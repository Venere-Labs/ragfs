# RAGFS Examples Configuration
# Copy this file to .env and fill in your API keys
#
# Usage:
#   cp .env.example .env
#   # Edit .env with your actual API keys
#   ./llamaindex_rag_pipeline.py query "What is RAGFS?"

# =============================================================================
# LLM Provider API Keys
# =============================================================================
# Only needed for the 'query' command - 'index' and 'search' work without them

# OpenAI (default provider)
# Get your key at: https://platform.openai.com/api-keys
OPENAI_API_KEY=sk-...

# Anthropic (Claude)
# Get your key at: https://console.anthropic.com/settings/keys
ANTHROPIC_API_KEY=sk-ant-...

# Ollama runs locally - no API key needed!
# Install: https://ollama.ai
# Then: ollama pull llama3.2

# =============================================================================
# Default Settings (optional)
# =============================================================================

# Default database path for all examples
# RAGFS_DB_PATH=./ragfs_db

# Default LLM provider: openai, anthropic, or ollama
# RAGFS_DEFAULT_PROVIDER=ollama

# Default model (leave empty to use provider defaults)
# OpenAI default: gpt-4o-mini
# Anthropic default: claude-sonnet-4-5-20250929
# Ollama default: llama3.2
# RAGFS_DEFAULT_MODEL=

# =============================================================================
# Quick Start with Ollama (no API keys needed!)
# =============================================================================
#
# 1. Install Ollama: https://ollama.ai
# 2. Pull a model: ollama pull llama3.2
# 3. Set default provider:
#    RAGFS_DEFAULT_PROVIDER=ollama
# 4. Run examples:
#    ./llamaindex_rag_pipeline.py index ./docs
#    ./llamaindex_rag_pipeline.py query "What is this project about?"
