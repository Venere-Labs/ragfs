# RAGFS Docker Stack v2 Environment Variables

# =============================================================================
# Ollama Configuration
# =============================================================================

# Model to use for LLM responses
OLLAMA_MODEL=llama3.2

# Alternative models (uncomment to use):
# OLLAMA_MODEL=mistral       # Faster, smaller
# OLLAMA_MODEL=llama3.1      # Larger, more capable
# OLLAMA_MODEL=codellama     # Optimized for code
# OLLAMA_MODEL=phi3          # Very small, fast

# =============================================================================
# Optional: Cloud LLM Providers
# =============================================================================

# Uncomment to use OpenAI instead of Ollama
# OPENAI_API_KEY=sk-...

# Uncomment to use Anthropic instead of Ollama
# ANTHROPIC_API_KEY=sk-ant-...

# =============================================================================
# Advanced Configuration (usually not needed)
# =============================================================================

# Path configurations (defaults work for Docker)
# RAGFS_DB_PATH=/data/index
# DOCUMENTS_PATH=/data/docs
# RAGFS_TRASH_PATH=/data/trash

# Ollama connection (default works for Docker)
# OLLAMA_BASE_URL=http://ollama:11434

# Chainlit settings
# CHAINLIT_HOST=0.0.0.0
# CHAINLIT_PORT=8000
